{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer\n",
    "vision and natural language processing, primarily developed by Facebook's AI Research lab (FAIR).\n",
    "It is free and open-source software released under the Modified BSD license. Although the Python interface is \n",
    "more polished and the primary focus of development, PyTorch also has a C++ interface.\n",
    "\n",
    "A number of pieces of Deep Learning software are built on top of PyTorch, including Tesla Autopilot,Uber's Pyro, \n",
    "HuggingFace's Transformers, PyTorch Lightning, and Catalyst.\n",
    "\n",
    "PyTorch provides two high-level features:\n",
    "    \n",
    "    1. Tensor computing (like NumPy) with strong acceleration via graphics processing units (GPU) \n",
    "    \n",
    "    2. Deep neural networks built on a tape-based automatic differentiation system \n",
    "    \n",
    "![](profiler.png)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:green '> Anaconda : To install PyTorch via Anaconda, use the following conda command: </span>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda install pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:blue '> Pip : To install PyTorch via pip, use the following command: </span>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification : To ensure that PyTorch was installed correctly, we can verify the installation by importing the torch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:green'>\n",
    "At its core, PyTorch is a library for processing tensors. A tensor is a number, vector, matrix, or any n-dimensional array. Let's create a tensor with a single number.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a scalar (zero-dimensional tensor)\n",
    "t0 = torch.tensor(2.)\n",
    "t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'> \n",
    "\"2.\" is a shorthand for 2.0. It is used to indicate to Python (and PyTorch) that you want to create a floating-point number. We can verify this by checking the dtype attribute of our tensor.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:green'> Let's try creating more complex tensors. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a vector (one-dimensional tensor)\n",
    "t1 = torch.tensor([1., 2, 3, 4])\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a matrix (two-dimensional tensor)\n",
    "t2 = torch.tensor([[5., 6], \n",
    "                   [7, 8], \n",
    "                   [9, 10]])\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11., 12., 13.],\n",
       "         [13., 14., 15.]],\n",
       "\n",
       "        [[15., 16., 17.],\n",
       "         [17., 18., 19.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a 3-dimensional tensor\n",
    "t3 = torch.tensor([\n",
    "    [[11, 12, 13], \n",
    "     [13, 14, 15]], \n",
    "    [[15, 16, 17], \n",
    "     [17, 18, 19.]]])\n",
    "t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:green'>\n",
    "Tensors can have any number of dimensions and different lengths along each dimension. We can inspect the length along each dimension using the .shape property of a tensor.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t0)\n",
    "t0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t1)\n",
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  6.],\n",
      "        [ 7.,  8.],\n",
      "        [ 9., 10.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t2)\n",
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[11., 12., 13.],\n",
      "         [13., 14., 15.]],\n",
      "\n",
      "        [[15., 16., 17.],\n",
      "         [17., 18., 19.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t3)\n",
    "t3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red '> Note that it's not possible to create tensors with an improper shape. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 1 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f24ab00f321a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m t4 = torch.tensor([[5., 6, 11], \n\u001b[1;32m      3\u001b[0m                    \u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                    [9, 10]])\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mt4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 2)"
     ]
    }
   ],
   "source": [
    "# Matrix (2-dimensional tensor)\n",
    "t4 = torch.tensor([[5., 6, 11], \n",
    "                   [7, 8], \n",
    "                   [9, 10]])\n",
    "t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red '> A ValueError is thrown because the lengths of the rows [5., 6, 11] and [7, 8] don't match. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tensor operations and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:green'> We can combine tensors with the usual arithmetic operations. Let's look at an example: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensors.\n",
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)\n",
    "x, w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:green'> We've created three tensors: x, w, and b, all numbers. w and b have an additional parameter requires_grad set to True. We'll see what it does in just a moment.\n",
    "Let's create a new tensor y by combining these tensors. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arithmetic operations\n",
    "y = w * x + b\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, y is a tensor with the value 4 * 3 + 5 = 17. What makes PyTorch unique is that we can automatically compute the derivative of y w.r.t. the tensors that have requires_grad set to True i.e. w and b. This feature of PyTorch is called autograd (automatic gradients).\n",
    "\n",
    "To compute the derivatives, we can invoke the .backward method on our result y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derivatives\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivatives of y with respect to the input tensors are stored in the .grad property of the respective tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: None\n",
      "dy/dw: tensor(3.)\n",
      "dy/db: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Display gradients\n",
    "print('dy/dx:', x.grad)\n",
    "print('dy/dw:', w.grad)\n",
    "print('dy/db:', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, dy/dw has the same value as x, i.e., 3, and dy/db has the value 1. Note that x.grad is None because x doesn't have requires_grad set to True.\n",
    "\n",
    "The \"grad\" in w.grad is short for gradient, which is another term for derivative. The term gradient is primarily used while dealing with vectors and matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Tensor functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from arithmetic operations, the torch module also contains many functions for creating and manipulating tensors. Let's look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42., 42.],\n",
       "        [42., 42.],\n",
       "        [42., 42.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor with a fixed value for every element\n",
    "t6 = torch.full((3, 2), 42)\n",
    "t6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor filled with the scalar value 0, with the shape defined by the variable argument size.\n",
    "t10=torch.zeros(2, 3)\n",
    "t10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Tensors must have same number of dimensions: got 3 and 2 at ../aten/src/TH/generic/THTensor.cpp:680",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7d3f32ec46be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Concatenate two tensors with compatible shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Tensors must have same number of dimensions: got 3 and 2 at ../aten/src/TH/generic/THTensor.cpp:680"
     ]
    }
   ],
   "source": [
    "# Concatenate two tensors with compatible shapes\n",
    "t7 = torch.cat((t3, t6))\n",
    "t7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't7' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c6a4b2a32669>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute the sin of each element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 't7' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute the sin of each element\n",
    "t8 = torch.sin(t7)\n",
    "t8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't8' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-36bb086a7112>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 't8' is not defined"
     ]
    }
   ],
   "source": [
    "t8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't8' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6a4b501835bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Change the shape of a tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 't8' is not defined"
     ]
    }
   ],
   "source": [
    "# Change the shape of a tensor\n",
    "t9 = t8.reshape(3, 2, 2)\n",
    "t9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a a 2-D tensor with ones on the diagonal and zeros elsewhere.\n",
    "t11=torch.eye(3)\n",
    "t11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [0.],\n",
       "         [0.]]), tensor([[0.],\n",
       "         [1.],\n",
       "         [0.]]), tensor([[0.],\n",
       "         [0.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splits the tensor into chunks. Each chunk is a view of the original tensor.\n",
    "torch.split(t11,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5462,  0.6273],\n",
       "        [ 0.4692,  2.8078],\n",
       "        [ 0.6966, -0.2060]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.0-D and 1-D tensors are returned as is.\n",
    "#When input is a 2-D tensor this is equivalent to transpose(input, 0, 1).\n",
    "t12 = torch.randn(2, 3)\n",
    "t12\n",
    "t13=torch.t(t12)\n",
    "t13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1949, 0.9946, 0.5184, 0.0449])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1)[0,1)\n",
    "torch.rand(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3379, 0.7742, 0.0211],\n",
       "        [0.2465, 0.6969, 0.8415]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computes the absolute value of each element in input.\n",
    "torch.abs(torch.tensor([-1, -2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5458, 0.6721, 0.9733, 0.8391])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([20.5458, 20.6721, 20.9733, 20.8391])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adds the scalar other to each element of the input input and returns a new resulting tensor.\n",
    "a = torch.rand(4)\n",
    "print(a)\n",
    "torch.add(a, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.5000, -0.2000])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computes the fractional portion of each element in input.\n",
    "torch.frac(torch.tensor([1, 2.5, -3.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns a new tensor with the exponential of the elements of the input tensor input.\n",
    "a = torch.rand(4)\n",
    "torch.exp(torch.tensor([0, torch.log(torch.tensor(2.))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about tensor operations here: https://pytorch.org/docs/stable/torch.html . Experiment with some more tensor functions and operations using the empty cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interoperability with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a popular open-source library used for mathematical and scientific computing in Python. It enables efficient operations on large multi-dimensional arrays and has a vast ecosystem of supporting libraries, including:\n",
    "\n",
    "Pandas for file I/O and data analysis\n",
    "\n",
    "Matplotlib for plotting and visualization\n",
    "\n",
    "OpenCV for image and video processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of reinventing the wheel, PyTorch interoperates well with Numpy to leverage its existing ecosystem of tools and libraries.\n",
    "\n",
    "Here's how we create an array in Numpy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 2], [3, 4.]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a Numpy array to a PyTorch tensor using torch.from_numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.from_numpy(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the numpy array and torch tensor have similar data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a PyTorch tensor to a Numpy array using the .numpy method of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a torch tensor to a numpy array\n",
    "z = y.numpy()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interoperability between PyTorch and Numpy is essential because most datasets you'll work with will likely be read and preprocessed as Numpy arrays.\n",
    "\n",
    "You might wonder why we need a library like PyTorch at all since Numpy already provides data structures and utilities for working with multi-dimensional numeric data. There are two main reasons:\n",
    "\n",
    "Autograd: The ability to automatically compute gradients for tensor operations is essential for training deep learning models.\n",
    "GPU support: While working with massive datasets and large models, PyTorch tensor operations can be performed efficiently using a Graphics Processing Unit (GPU). Computations that might typically take hours can be completed within minutes using GPUs.\n",
    "We'll leverage both these features of PyTorch extensively in in the the coming sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'cfloat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-cad9ccbb21a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'cfloat'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience and visualization, we will only use two features in this notebook, so we are still able to plot them together with the target class and decision boundary\n",
    "\n",
    "First we will create some artificial data:\n",
    "\n",
    "-\n",
    "m\n",
    "1\n",
    "=\n",
    "10\n",
    " examples for class 0 -\n",
    "m\n",
    "2\n",
    "=\n",
    "15\n",
    " examples for class 1 -\n",
    "n\n",
    "=\n",
    "2\n",
    " features for each example\n",
    "\n",
    "No exercise yet, just execute the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = 10\n",
    "m2 = 15\n",
    "m = m1 + m2\n",
    "n = 2\n",
    "X = np.ndarray((m,n))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.zeros((m))\n",
    "y[m1:] = y[m1:] + 1.0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Execute this to generate linearly sperable data\n",
    "def x2_function_class_0(x):\n",
    "    return -x*2 + 2\n",
    "\n",
    "def x2_function_class_1(x):\n",
    "    return -x*2 + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Execute this to generate NOT linearly sperable data\n",
    "def x2_function_class_0(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def x2_function_class_1(x):\n",
    "    return np.sin(x) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_min = -5\n",
    "x1_max = +5\n",
    "\n",
    "X[:m1,0] = np.linspace(x1_min, x1_max, m1)\n",
    "X[m1:,0] = np.linspace(x1_min+0.5, x1_max-0.2, m2)\n",
    "X[:m1,1] = x2_function_class_0(X[:m1,0])\n",
    "X[m1:,1] = x2_function_class_1(X[m1:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data():\n",
    "    plt.scatter(X[:m1,0], X[:m1,1], alpha=0.5, label='class 0 train data')\n",
    "    plt.scatter(X[m1:,0], X[m1:,1], alpha=0.5, label='class 1 train data')\n",
    "\n",
    "    plt.plot(x1_line, x2_line_class_0, alpha=0.2, label='class 0 true target func')\n",
    "    plt.plot(x1_line, x2_line_class_1, alpha=0.2, label='class 1 true target func')\n",
    "    plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1_line = np.linspace(x1_min, x1_max, 100)\n",
    "x2_line_class_0 = x2_function_class_0(x1_line)\n",
    "x2_line_class_1 = x2_function_class_1(x1_line)    \n",
    "\n",
    "plot_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the Data to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_tensor= torch.tensor(X)\n",
    "y_tensor= torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 2])\n",
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_tensor.shape) ### should be [25,2]\n",
    "print(y_tensor.shape) ### should be [25,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy wont be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Behind the scenes, Tensors can keep track of a computational graph and gradients, but theyre also useful as a generic tool for scientific computing.\n",
    "\n",
    "Also unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you simply need to specify the correct device.\n",
    "\n",
    "Here we use PyTorch Tensors to fit a third order polynomial to sine function. Like the numpy example above we need to manually implement the forward and backward passes through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 173.5841064453125\n",
      "199 124.26693725585938\n",
      "299 89.7569351196289\n",
      "399 65.59271240234375\n",
      "499 48.662845611572266\n",
      "599 36.794776916503906\n",
      "699 28.470582962036133\n",
      "799 22.62906265258789\n",
      "899 18.527790069580078\n",
      "999 15.646990776062012\n",
      "1099 13.62258529663086\n",
      "1199 12.199406623840332\n",
      "1299 11.198508262634277\n",
      "1399 10.494311332702637\n",
      "1499 9.998703956604004\n",
      "1599 9.649775505065918\n",
      "1699 9.404050827026367\n",
      "1799 9.23094367980957\n",
      "1899 9.108966827392578\n",
      "1999 9.022991180419922\n",
      "Result: y = -0.014828972518444061 + 0.8537359833717346 x + 0.002558246487751603 x^2 + -0.09290297329425812 x^3\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1284.0\n",
      "199 852.4677124023438\n",
      "299 566.993408203125\n",
      "399 378.1315612792969\n",
      "499 253.1829833984375\n",
      "599 170.51455688476562\n",
      "699 115.81796264648438\n",
      "799 79.62669372558594\n",
      "899 55.67914962768555\n",
      "999 39.83236312866211\n",
      "1099 29.345722198486328\n",
      "1199 22.405611038208008\n",
      "1299 17.812328338623047\n",
      "1399 14.77219009399414\n",
      "1499 12.759838104248047\n",
      "1599 11.42774486541748\n",
      "1699 10.545905113220215\n",
      "1799 9.962041854858398\n",
      "1899 9.575450897216797\n",
      "1999 9.319458961486816\n",
      "Result: y = -0.0038628641050308943 + 0.8782362937927246 x + 0.0006664061802439392 x^2 + -0.09638793021440506 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Defining new autograd functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Under the hood, each primitive autograd operator is really two functions that operate on Tensors. The forward function computes output Tensors from input Tensors. The backward function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value.\n",
    "\n",
    "In PyTorch we can easily define our own autograd operator by defining a subclass of torch.autograd.Function and implementing the forward and backward functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data.\n",
    "\n",
    "In this example we define our model as $y=a+bP_3(c+dx)$ instead of y=a+bx+cx^2+dx^3, where $P_3(x)=1/2(5x^33x)$ is the Legendre polynomial of degree three. We write our own custom autograd function for computing forward and backward of P3, and use it to implement our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 209.9583282470703\n",
      "199 144.6602020263672\n",
      "299 100.7025146484375\n",
      "399 71.03520965576172\n",
      "499 50.978511810302734\n",
      "599 37.403133392333984\n",
      "699 28.20686912536621\n",
      "799 21.973180770874023\n",
      "899 17.7457275390625\n",
      "999 14.877889633178711\n",
      "1099 12.931767463684082\n",
      "1199 11.610918998718262\n",
      "1299 10.714247703552246\n",
      "1399 10.105476379394531\n",
      "1499 9.69210433959961\n",
      "1599 9.411375045776367\n",
      "1699 9.220744132995605\n",
      "1799 9.091285705566406\n",
      "1899 9.003362655639648\n",
      "1999 8.943641662597656\n",
      "Result: y = 4.270284570395688e-09 + -2.208526849746704 * P3(-1.3157669398466965e-09 + 0.2554861009120941 x)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For this example, we need\n",
    "# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n",
    "# not too far from the correct result to ensure convergence.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # P3 using our custom autograd operation.\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of data transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Library\n",
    "PyTorch code is simple. It is easy to understand, and you use the library instantly. For example, take a look at the code snippet below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer = torch.nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "     def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 40, 5)\n",
    "        self.fc1 = nn.Linear(320, 10)\n",
    "\n",
    "def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "net = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize our data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(100)\n",
    "y = np.sin(x) * np.power(x,3) + 3*x + np.random.rand(100)*0.8\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4241],\n",
      "        [0.9337],\n",
      "        [0.5541],\n",
      "        [0.0885],\n",
      "        [0.6881],\n",
      "        [0.6729],\n",
      "        [0.8666],\n",
      "        [0.1363],\n",
      "        [0.0912],\n",
      "        [0.7828],\n",
      "        [0.6578],\n",
      "        [0.5231],\n",
      "        [0.6829],\n",
      "        [0.9468],\n",
      "        [0.2249],\n",
      "        [0.3882],\n",
      "        [0.8935],\n",
      "        [0.4958],\n",
      "        [0.0100],\n",
      "        [0.0838],\n",
      "        [0.9481],\n",
      "        [0.7307],\n",
      "        [0.4451],\n",
      "        [0.1311],\n",
      "        [0.6129],\n",
      "        [0.2620],\n",
      "        [0.7210],\n",
      "        [0.8706],\n",
      "        [0.7696],\n",
      "        [0.3035],\n",
      "        [0.6609],\n",
      "        [0.1307],\n",
      "        [0.2332],\n",
      "        [0.3154],\n",
      "        [0.9014],\n",
      "        [0.6125],\n",
      "        [0.0408],\n",
      "        [0.9011],\n",
      "        [0.8677],\n",
      "        [0.0532],\n",
      "        [0.5530],\n",
      "        [0.4010],\n",
      "        [0.1520],\n",
      "        [0.2551],\n",
      "        [0.6000],\n",
      "        [0.3602],\n",
      "        [0.3719],\n",
      "        [0.7340],\n",
      "        [0.9415],\n",
      "        [0.2986],\n",
      "        [0.3687],\n",
      "        [0.2909],\n",
      "        [0.0492],\n",
      "        [0.6795],\n",
      "        [0.3069],\n",
      "        [0.7382],\n",
      "        [0.1252],\n",
      "        [0.7616],\n",
      "        [0.8295],\n",
      "        [0.8574],\n",
      "        [0.2756],\n",
      "        [0.2493],\n",
      "        [0.2317],\n",
      "        [0.3730],\n",
      "        [0.4850],\n",
      "        [0.4610],\n",
      "        [0.8847],\n",
      "        [0.1620],\n",
      "        [0.1197],\n",
      "        [0.8845],\n",
      "        [0.5085],\n",
      "        [0.2532],\n",
      "        [0.0738],\n",
      "        [0.5626],\n",
      "        [0.6901],\n",
      "        [0.4182],\n",
      "        [0.5641],\n",
      "        [0.3076],\n",
      "        [0.8754],\n",
      "        [0.1557],\n",
      "        [0.2895],\n",
      "        [0.3127],\n",
      "        [0.5032],\n",
      "        [0.7173],\n",
      "        [0.9954],\n",
      "        [0.3596],\n",
      "        [0.8724],\n",
      "        [0.6888],\n",
      "        [0.4549],\n",
      "        [0.6411],\n",
      "        [0.5426],\n",
      "        [0.4162],\n",
      "        [0.4950],\n",
      "        [0.4741],\n",
      "        [0.0583],\n",
      "        [0.1042],\n",
      "        [0.4578],\n",
      "        [0.0516],\n",
      "        [0.3374],\n",
      "        [0.8459]]) tensor([[1.4939],\n",
      "        [3.5246],\n",
      "        [1.9951],\n",
      "        [0.8876],\n",
      "        [3.0231],\n",
      "        [2.3027],\n",
      "        [3.6111],\n",
      "        [0.9109],\n",
      "        [1.0604],\n",
      "        [2.9459],\n",
      "        [2.3562],\n",
      "        [2.3703],\n",
      "        [2.6333],\n",
      "        [3.8926],\n",
      "        [1.4658],\n",
      "        [1.3306],\n",
      "        [3.5818],\n",
      "        [2.1359],\n",
      "        [0.0461],\n",
      "        [0.2961],\n",
      "        [3.6365],\n",
      "        [3.1201],\n",
      "        [2.0731],\n",
      "        [0.6580],\n",
      "        [2.1806],\n",
      "        [0.8982],\n",
      "        [2.8586],\n",
      "        [3.8135],\n",
      "        [2.8476],\n",
      "        [1.0006],\n",
      "        [2.2465],\n",
      "        [0.7849],\n",
      "        [0.7701],\n",
      "        [1.2044],\n",
      "        [3.8859],\n",
      "        [2.6983],\n",
      "        [0.2292],\n",
      "        [3.6238],\n",
      "        [3.3229],\n",
      "        [0.3387],\n",
      "        [2.2635],\n",
      "        [1.3796],\n",
      "        [1.0479],\n",
      "        [1.2417],\n",
      "        [1.9773],\n",
      "        [1.5738],\n",
      "        [1.1493],\n",
      "        [3.1030],\n",
      "        [3.5166],\n",
      "        [1.2398],\n",
      "        [1.3302],\n",
      "        [1.1546],\n",
      "        [0.8622],\n",
      "        [2.5110],\n",
      "        [1.7104],\n",
      "        [2.7246],\n",
      "        [1.1757],\n",
      "        [3.3518],\n",
      "        [3.3822],\n",
      "        [3.2192],\n",
      "        [1.0964],\n",
      "        [1.3945],\n",
      "        [1.2992],\n",
      "        [1.3558],\n",
      "        [2.0648],\n",
      "        [1.5036],\n",
      "        [3.6410],\n",
      "        [0.9146],\n",
      "        [0.7544],\n",
      "        [3.4959],\n",
      "        [1.9534],\n",
      "        [1.0676],\n",
      "        [0.5958],\n",
      "        [1.8160],\n",
      "        [2.3951],\n",
      "        [1.8097],\n",
      "        [2.1443],\n",
      "        [1.7190],\n",
      "        [3.9312],\n",
      "        [0.5192],\n",
      "        [1.5210],\n",
      "        [1.0997],\n",
      "        [1.8019],\n",
      "        [2.7334],\n",
      "        [4.5188],\n",
      "        [1.6760],\n",
      "        [3.3702],\n",
      "        [2.3629],\n",
      "        [1.9567],\n",
      "        [2.1092],\n",
      "        [2.0454],\n",
      "        [1.6699],\n",
      "        [1.5766],\n",
      "        [1.8289],\n",
      "        [0.2511],\n",
      "        [0.8995],\n",
      "        [1.4753],\n",
      "        [0.6475],\n",
      "        [1.7115],\n",
      "        [3.4757]])\n"
     ]
    }
   ],
   "source": [
    "# convert numpy array to tensor in shape of input size\n",
    "x = torch.from_numpy(x.reshape(-1,1)).float()\n",
    "y = torch.from_numpy(y.reshape(-1,1)).float()\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 610.6955458231203\n",
      "199 409.2038509062596\n",
      "299 275.2675411801156\n",
      "399 186.206715368958\n",
      "499 126.9646462944247\n",
      "599 87.54272787166946\n",
      "699 61.29941638278668\n",
      "799 43.82178609636021\n",
      "899 32.176784249697846\n",
      "999 24.41431100855865\n",
      "1099 19.237348557289955\n",
      "1199 15.782926381046614\n",
      "1299 13.4766423143281\n",
      "1399 11.93601010236117\n",
      "1499 10.906226363240988\n",
      "1599 10.21746890311655\n",
      "1699 9.756499174529498\n",
      "1799 9.44777043337985\n",
      "1899 9.240854857958212\n",
      "1999 9.102072659659923\n",
      "Result: y = -0.009001524350991857 + 0.8425674708978899 x + 0.0015529127365578345 x^2 + -0.09131435027138864 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now well set up TensorBoard, importing tensorboard from torch.utils and defining a SummaryWriter, our key object for writing information to TensorBoard.\n",
    "\n",
    "TensorBoard: TensorFlow's visualization toolkit\n",
    "\n",
    "TensorBoard provides the visualization and tooling needed for machine learning experimentation:\n",
    " ->Tracking and visualizing metrics such as loss and accuracy\n",
    " \n",
    " ->Visualizing the model graph (ops and layers)\n",
    " \n",
    " ->Viewing histograms of weights, biases, or other tensors as they change over time\n",
    " \n",
    " ->Projecting embeddings to a lower dimensional space\n",
    " \n",
    " ->Displaying images, text, and audio data\n",
    " \n",
    " ->Profiling TensorFlow programs\n",
    " \n",
    "And much more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZING MODELS, DATA, AND TRAINING WITH TENSORBOARD\n",
    "In the 60 Minute Blitz, we show you how to load in data, feed it through a model we define as a subclass of nn.Module, train this model on training data, and test it on test data. To see whats happening, we print out some statistics as the model is training to get a sense for whether training is progressing. However, we can do much better than that: PyTorch integrates with TensorBoard, a tool designed for visualizing the results of neural network training runs. This tutorial illustrates some of its functionality, using the Fashion-MNIST dataset which can be read into PyTorch using torchvision.datasets.\n",
    "\n",
    "In this tutorial, well learn how to:\n",
    "\n",
    "    Read in data and with appropriate transforms (nearly identical to the prior tutorial).\n",
    "    \n",
    "    Set up TensorBoard.\n",
    "    \n",
    "    Write to TensorBoard.\n",
    "\n",
    "    Inspect a model architecture using TensorBoard.\n",
    "    \n",
    "    Use TensorBoard to create interactive versions of the visualizations we created in last tutorial, with less code\n",
    "    Specifically, on point #5, well see:\n",
    "\n",
    "A couple of ways to inspect our training data\n",
    "How to track our models performance as it trains\n",
    "How to assess our models performance once it is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "# constant for classes\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well define a similar model architecture from that tutorial, making only minor modifications to account for the fact that the images are now one channel instead of three and 28x28 instead of 32x32:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well define the same optimizer and criterion from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TensorBoard setup\n",
    "Now well set up TensorBoard, importing tensorboard from torch.utils and defining a SummaryWriter, our key object for writing information to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this line alone creates a runs/fashion_mnist_experiment_1 folder.\n",
    "\n",
    "# 2. Writing to TensorBoard\n",
    "\n",
    "Now lets write an image to our TensorBoard - specifically, a grid - using make_grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 1943), started 1:32:50 ago. (Use '!kill 1943' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-534ac4599a4c1047\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-534ac4599a4c1047\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to use TensorBoard! This example, however, could be done in a Jupyter Notebook - where TensorBoard really excels is in creating interactive visualizations. Well cover one of those next, and several more by the end of the tutorial.\n",
    "\n",
    "# 3. Inspect the model using TensorBoard\n",
    "One of TensorBoards strengths is its ability to visualize complex model structures. Lets visualize the model we built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upon refreshing TensorBoard you should see a Graphs tab that looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and double click on Net to see it expand, seeing a detailed view of the individual operations that make up the model.\n",
    "\n",
    "TensorBoard has a very handy feature for visualizing high dimensional data such as image data in a lower dimensional space; well cover this next.\n",
    "\n",
    "# 4. Adding a Projector to TensorBoard\n",
    "We can visualize the lower dimensional representation of higher dimensional data via the add_embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the Projector tab of TensorBoard, you can see these 100 images - each of which is 784 dimensional - projected down into three dimensional space. Furthermore, this is interactive: you can click and drag to rotate the three dimensional projection. Finally, a couple of tips to make the visualization easier to see: select color: label on the top left, as well as enabling night mode, which will make the images easier to see since their background is white:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 1943), started 1:09:23 ago. (Use '!kill 1943' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4b87b936583c1128\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4b87b936583c1128\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now weve thoroughly inspected our data, lets show how TensorBoard can make tracking model training and evaluation clearer, starting with training.\n",
    "\n",
    "# 5. Tracking model training with TensorBoard\n",
    "In the previous example, we simply printed the models running loss every 2000 iterations. Now, well instead log the running loss to TensorBoard, along with a view into the predictions the model is making via the plot_classes_preds function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets train the model using the same model training code from the prior tutorial, but writing results to TensorBoard every 1000 batches instead of printing to console; this is done using the add_scalar function.\n",
    "\n",
    "In addition, as we train, well generate an image showing the models predictions vs. the actual results on the four images included in that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # every 1000 mini-batches...\n",
    "\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 1000,\n",
    "                            epoch * len(trainloader) + i)\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "            writer.add_figure('predictions vs. actuals',\n",
    "                            plot_classes_preds(net, inputs, labels),\n",
    "                            global_step=epoch * len(trainloader) + i)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 0).\n",
       "Contents of stdout:\n",
       "2.4.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'import numpy as np\\nimport tensorflow as tf\\nimport tensorboard as tb\\ntf.io.gfile = tb.compat.tensorflow_stub.io.gfile\\nfrom torch.utils.tensorboard import SummaryWriter\\n\\nvectors = np.array([[0,0,1], [0,1,0], [1,0,0], [1,1,1]])\\nmetadata = ['001', '010', '100', '111']  # labels\\nwriter = SummaryWriter()\\nwriter.add_embedding(vectors, metadata)\\nwriter.close()\\n%load_ext tensorboard\\n%tensorboard --logdir=runs\""
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "vectors = np.array([[0,0,1], [0,1,0], [1,0,0], [1,1,1]])\n",
    "metadata = ['001', '010', '100', '111']  # labels\n",
    "writer = SummaryWriter()\n",
    "writer.add_embedding(vectors, metadata)\n",
    "writer.close()\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs'''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
