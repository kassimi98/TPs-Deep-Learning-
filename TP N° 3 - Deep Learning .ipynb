{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Practical Deep Learning Tutorial with PyTorch - Tutorial N° 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2020-2021\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Built ADALINE model using the nn.Module class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Adaline, self).__init__()\n",
    "        self.linear = torch.nn.Linear(num_features , 1) # une transformation lineare de la forme X.(W.transposé)\n",
    "                                                        #X matrice de donnes qui a (num_features) variables\n",
    "                                                        # chaque donne sera multiplie par un vecteur de poids \n",
    "                                                        # de taille (1,num_features), ca donne un veteur de taille\n",
    "                                                        # (num_features,1)\n",
    "        self.linear.weight.detach().zero_()  # changer les poids aleatoire à zero (pour l'initialisation)\n",
    "        self.linear.bias.detach().zero_()  #meme chose pour le bias\n",
    "        \n",
    "    def forward(self, x):\n",
    "        activations = self.linear(x)\n",
    "        return activations.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using 'iris.txt', create a binary datasets in 2-D : The last 100 instances of iris described only by the 2nd and 3rd features\n",
    "    \n",
    "    Split the dataset into traing and test sets (70%,30%) \n",
    "\n",
    "    Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('C:/Users/PC/Desktop/iris.txt', index_col=None, header=None)    #lire le fichier iris\n",
    "df.columns = ['x1', 'x2', 'x3', 'x4', 'y'] #renommer les colonnes\n",
    "df = df.iloc[50:150]   #prendre que la 100 dernière données de la base, donc que les donnees des 2 dernieres classes\n",
    "df['y'] = df['y'].apply(lambda x: 0 if x == 'Iris-versicolor' else 1) #coder la 2e classe par 0 la 3e par 1\n",
    "\n",
    "\n",
    "# Assign features and target\n",
    "\n",
    "X = torch.tensor(df[['x2', 'x3']].values, dtype=torch.float) #ne considerer que la 2e et 3e variables, c'est notre tensor X\n",
    "y = torch.tensor(df['y'].values, dtype=torch.int) #le tensor y correeponds aux labels y\n",
    "\n",
    "# Shuffling & train/test split\n",
    "torch.manual_seed(123)\n",
    "shuffle_idx = torch.randperm(y.size(0), dtype=torch.long)  # melanger les indices \n",
    "X, y = X[shuffle_idx], y[shuffle_idx]  # melanger les donnes, ca garde la corrspondance entre une donnée et son label\n",
    "percent70 = int(shuffle_idx.size(0)*0.7)\n",
    "X_train, X_test = X[shuffle_idx[:percent70]], X[shuffle_idx[percent70:]]  #70 premiers points pour training \n",
    "y_train, y_test = y[shuffle_idx[:percent70]], y[shuffle_idx[percent70:]]  # 30 dernieres données pour le test\n",
    "\n",
    "# Normalize (mean zero, unit variance)\n",
    "\n",
    "mu, sigma = X_train.mean(dim=0), X_train.std(dim=0) #normalization, soustraire la moyenne diviser par lecarte type\n",
    "X_train =  (X_train - mu) / sigma   \n",
    "X_test = (X_test - mu) / sigma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adaline(\n",
       "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Adaline(num_features=X_train.size(1))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the model : we will use MSELoss (mean squared error (squared L2 norm)) as loss function. The optimizer is SGD (Stochastic Gradient Descent) with learning rate 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, num_epochs, learning_rate, seed):\n",
    "    cost = []\n",
    "    torch.manual_seed(seed)\n",
    "    optimizer =  torch.optim.SGD(model.parameters() , lr=learning_rate)\n",
    "    losst = torch.nn.MSELoss()\n",
    "    for e in range(num_epochs):\n",
    "        yhat = model.forward(x)  #calcul yhat\n",
    "        loss =  losst(y , yhat)  #calcul the loss function using MSE\n",
    "        print(loss.item() , 'loss')\n",
    "        optimizer.zero_grad()                   # set the gradients to zero\n",
    "        loss.backward()                         # calculer le gradients\n",
    "        optimizer.step()                        # mise a jour des poids ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48571428656578064 loss\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n",
      "0.4699629247188568 loss\n",
      "0.45489606261253357 loss\n",
      "0.4404822289943695 loss\n",
      "0.42669153213500977 loss\n",
      "0.4134954810142517 loss\n",
      "0.40086689591407776 loss\n",
      "0.3887799382209778 loss\n",
      "0.37721002101898193 loss\n",
      "0.3661336600780487 loss\n",
      "0.35552868247032166 loss\n",
      "0.3453736901283264 loss\n",
      "0.33564844727516174 loss\n",
      "0.3263336420059204 loss\n",
      "0.317410945892334 loss\n",
      "0.30886268615722656 loss\n",
      "0.30067235231399536 loss\n",
      "0.29282382130622864 loss\n",
      "0.2853020429611206 loss\n",
      "0.27809247374534607 loss\n",
      "0.2711813747882843 loss\n",
      "0.2645554840564728 loss\n",
      "0.2582024037837982 loss\n",
      "0.2521100640296936 loss\n",
      "0.24626705050468445 loss\n",
      "0.24066250026226044 loss\n",
      "0.23528602719306946 loss\n",
      "0.2301277220249176 loss\n",
      "0.2251780778169632 loss\n",
      "0.22042809426784515 loss\n",
      "0.21586920320987701 loss\n",
      "0.21149314939975739 loss\n",
      "0.20729205012321472 loss\n",
      "0.2032584398984909 loss\n",
      "0.19938518106937408 loss\n",
      "0.1956653892993927 loss\n",
      "0.19209259748458862 loss\n",
      "0.18866051733493805 loss\n",
      "0.1853632628917694 loss\n",
      "0.18219506740570068 loss\n",
      "0.17915058135986328 loss\n",
      "0.17622457444667816 loss\n",
      "0.17341205477714539 loss\n",
      "0.1707083284854889 loss\n",
      "0.16810885071754456 loss\n",
      "0.16560928523540497 loss\n",
      "0.16320547461509705 loss\n",
      "0.1608934998512268 loss\n",
      "0.15866954624652863 loss\n",
      "0.15653002262115479 loss\n",
      "0.15447144210338593 loss\n",
      "0.1524905264377594 loss\n",
      "0.15058404207229614 loss\n",
      "0.14874902367591858 loss\n",
      "0.14698255062103271 loss\n",
      "0.1452818512916565 loss\n",
      "0.14364425837993622 loss\n",
      "0.14206725358963013 loss\n",
      "0.14054837822914124 loss\n",
      "0.13908536732196808 loss\n",
      "0.1376759111881256 loss\n",
      "0.1363179087638855 loss\n",
      "0.13500936329364777 loss\n",
      "0.13374821841716766 loss\n",
      "0.13253268599510193 loss\n",
      "0.1313609480857849 loss\n",
      "0.13023130595684052 loss\n",
      "0.12914206087589264 loss\n",
      "0.1280916929244995 loss\n",
      "0.1270786076784134 loss\n",
      "0.12610146403312683 loss\n",
      "0.12515880167484283 loss\n",
      "0.12424926459789276 loss\n",
      "0.1233716681599617 loss\n",
      "0.12252470850944519 loss\n",
      "0.1217072606086731 loss\n",
      "0.12091819196939468 loss\n",
      "0.12015638500452042 loss\n",
      "0.1194208413362503 loss\n",
      "0.11871055513620377 loss\n",
      "0.11802458018064499 loss\n",
      "0.11736200749874115 loss\n",
      "0.11672195792198181 loss\n",
      "0.11610356718301773 loss\n",
      "0.11550606042146683 loss\n",
      "0.11492867022752762 loss\n",
      "0.11437057703733444 loss\n",
      "0.11383116245269775 loss\n",
      "0.11330968886613846 loss\n",
      "0.11280549317598343 loss\n",
      "0.11231797188520432 loss\n",
      "0.11184649169445038 loss\n",
      "0.11139050871133804 loss\n",
      "0.11094939708709717 loss\n",
      "0.11052266508340836 loss\n",
      "0.11010980606079102 loss\n",
      "0.10971028357744217 loss\n",
      "0.1093236431479454 loss\n",
      "0.10894942283630371 loss\n",
      "0.10858717560768127 loss\n",
      "0.10823649168014526 loss\n",
      "0.10789693146944046 loss\n",
      "0.10756812989711761 loss\n",
      "0.10724970698356628 loss\n",
      "0.10694129019975662 loss\n",
      "0.10664254426956177 loss\n",
      "0.10635311156511307 loss\n",
      "0.10607270151376724 loss\n",
      "0.10580097138881683 loss\n",
      "0.10553761571645737 loss\n",
      "0.10528238862752914 loss\n",
      "0.1050349771976471 loss\n",
      "0.10479512065649033 loss\n",
      "0.10456258058547974 loss\n",
      "0.10433709621429443 loss\n",
      "0.10411842912435532 loss\n",
      "0.10390635579824448 loss\n",
      "0.10370063781738281 loss\n",
      "0.10350107401609421 loss\n",
      "0.10330746322870255 loss\n",
      "0.1031196117401123 loss\n",
      "0.10293731838464737 loss\n",
      "0.10276041179895401 loss\n",
      "0.1025886982679367 loss\n",
      "0.10242202132940292 loss\n",
      "0.10226022452116013 loss\n",
      "0.10210311412811279 loss\n",
      "0.10195057839155197 loss\n",
      "0.10180244594812393 loss\n",
      "0.10165857523679733 loss\n",
      "0.10151883959770203 loss\n",
      "0.10138311237096786 loss\n",
      "0.10125124454498291 loss\n",
      "0.1011231318116188 loss\n",
      "0.1009986475110054 loss\n",
      "0.10087767988443375 loss\n",
      "0.10076010972261429 loss\n",
      "0.10064584016799927 loss\n",
      "0.10053477436304092 loss\n",
      "0.1004268079996109 loss\n",
      "0.10032182186841965 loss\n",
      "0.10021977126598358 loss\n"
     ]
    }
   ],
   "source": [
    "train(model, X_train, y_train.float(),num_epochs=142,learning_rate=0.01,seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5000)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean( torch.tensor([1.,2.,3.,4.]) , dtype=torch.float32 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the model accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 92.86\n",
      "Test Accuracy: 93.33\n",
      "Weights Parameter containing:\n",
      "tensor([[-0.0195,  0.3655]], requires_grad=True)\n",
      "Bias Parameter containing:\n",
      "tensor([0.4581], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def custom_where(cond, x_1, x_2):\n",
    "    \n",
    "    return (cond * x_1) + (torch.logical_not(cond) * x_2)\n",
    "\n",
    "train_pred = model.forward(X_train)\n",
    "\n",
    "train_acc = torch.mean(\n",
    "\n",
    "    (custom_where(train_pred > 0.5, 1, 0).int() == y_train).float()\n",
    "\n",
    ")\n",
    "test_pred = model.forward(X_test)\n",
    "test_acc = torch.mean((custom_where(test_pred > 0.5, 1, 0).int() == y_test).float())\n",
    "print('Training Accuracy: %.2f' % (train_acc*100))\n",
    "print('Test Accuracy: %.2f' % (test_acc*100))\n",
    "print('Weights', model.linear.weight)\n",
    "print('Bias', model.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Built a Perceptron model using nn.Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def custom_where(cond, x_1, x_2):\n",
    "        return (cond * x_1) + (torch.logical_not(cond) * x_2)\n",
    "\n",
    "class Perceptron():\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weights = torch.zeros(num_features, 1, dtype=torch.float32, device=device)\n",
    "        self.bias = torch.zeros(1, dtype=torch.float32, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        predictions = torch.add( torch.mm(x , self.weights) , self.bias )\n",
    "        predictions = custom_where(predictions > 0. , 1 , 0).float()\n",
    "        return predictions\n",
    "        \n",
    "    def backward(self, x, y):\n",
    "        predictions = self.forward(x)\n",
    "        errors = y - predictions\n",
    "        return errors\n",
    "        \n",
    "    def train(self, x, y, epochs):\n",
    "        for e in range(epochs):\n",
    "            for i in range(y.shape[0]):\n",
    "                errors = self.backward( x[i].view(1 , self.num_features) , y[i] ).view(-1)\n",
    "                self.weights += (errors * x[i]).view(self.num_features, 1)  \n",
    "                self.bias += errors \n",
    "                \n",
    "    def evaluate(self, x, y):\n",
    "        predictions = self.forward(x).reshape(-1)\n",
    "        accuracy = torch.sum(predictions == y).float() / y.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Load the 'perceptron_toydata' dataset\n",
    "\n",
    "    Split the dataset into train and test sets\n",
    "    \n",
    "    Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label counts: tensor([50, 50])\n",
      "X.shape: torch.Size([100, 2])\n",
      "y.shape: torch.Size([100])\n",
      "torch.Size([70, 2])\n",
      "torch.Size([70])\n",
      "torch.Size([30, 2])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/PC/Desktop/toy_data.txt', index_col=None, header=None , delimiter='\\t')\n",
    "df.columns = ['x1', 'x2', 'y']\n",
    "X = torch.tensor(df[['x1', 'x2']].values, dtype=torch.float) \n",
    "y = torch.tensor(df['y'].values, dtype=torch.int) \n",
    "print('Class label counts:', torch.bincount(y))\n",
    "print('X.shape:', X.shape)\n",
    "print('y.shape:', y.shape)\n",
    "\n",
    "# Shuffling & train/test split\n",
    "shuffle_idx = torch.randperm(y.size(0), dtype=torch.long)\n",
    "X, y = X[shuffle_idx], y[shuffle_idx]\n",
    "percent70 = int(shuffle_idx.size(0)*0.7)\n",
    "X_train, X_test = X[shuffle_idx[:percent70]], X[shuffle_idx[percent70:]]\n",
    "y_train, y_test = y[shuffle_idx[:percent70]], y[shuffle_idx[percent70:]]\n",
    "# Normalize (mean zero, unit variance)\n",
    "mu, sigma = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_test = (X_test - mu) / sigma\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Train the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "Model parameters:\n",
      "  Weights: tensor([[2.3745],\n",
      "        [0.9723]])\n",
      "  Bias: tensor([-1.])\n"
     ]
    }
   ],
   "source": [
    "model = Perceptron(num_features=2)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "\n",
    "model.train(X_train_tensor, y_train_tensor, epochs=5)\n",
    "\n",
    "print('Model parameters:')\n",
    "print('  Weights: %s' % model.weights)\n",
    "print('  Bias: %s' % model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. evaluate the model (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 96.67%\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "\n",
    "test_acc = model.evaluate(X_test_tensor, y_test_tensor)\n",
    "print('Test set accuracy: %.2f%%' % (test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the single-layer perceptron, the Multi Layer Perceptron models have hidden layers\n",
    "between the input and the output layers. After every hidden layer, an activation function \n",
    "is applied to introduce non-linearity. \n",
    "\n",
    "9. Built a simple Multi Layer Perceptron model withe one hidden layer. \n",
    "After the hidden layer, we will use ReLU as activation before the information is sent to the output layer.\n",
    "As an output activation function, we will use Sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, num_features,num_hidden_1):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.fct = nn.Linear(num_features , num_hidden_1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fct2 = nn.Linear(num_hidden_1 , 1)\n",
    "        self.sgm = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fct(x))\n",
    "        out = self.sgm(self.fct2(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a random datasets and assign binary labels {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 2])\n",
      "torch.Size([40])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "def blob_label(y, label, loc): # assign labels\n",
    "    target = np.copy(y)\n",
    "    for l in loc:\n",
    "        target[y == l] = label\n",
    "    return target\n",
    "x_train, y_train = make_blobs(n_samples=40, n_features=2, cluster_std=1.5, shuffle=True)\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.FloatTensor(blob_label(y_train, 0, [0]))\n",
    "y_train = torch.FloatTensor(blob_label(y_train, 1, [1,2,3]))\n",
    "x_test, y_test = make_blobs(n_samples=10, n_features=2, cluster_std=1.5, shuffle=True)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.FloatTensor(blob_label(y_test, 0, [0]))\n",
    "y_test = torch.FloatTensor(blob_label(y_test, 1, [1,2,3]))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Define the model with input dimension 2 and hidden dimension 10. \n",
    "Since the task is to classify binary labels, we can use as criterion BCELoss (Binary Cross Entropy Loss) : loss function.\n",
    "The optimizer is SGD (Stochastic Gradient Descent) with learning rate 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(2, 10)\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters() , lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Check the test loss before the model training and compare it with the test loss after the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training 1.057615041732788\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "before_train = criterion(y_pred.squeeze(), y_test)\n",
    "print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.6781933903694153\n",
      "Epoch 1: train loss: 0.6343165636062622\n",
      "Epoch 2: train loss: 0.5938801765441895\n",
      "Epoch 3: train loss: 0.5567026734352112\n",
      "Epoch 4: train loss: 0.5231706500053406\n",
      "Epoch 5: train loss: 0.4925197660923004\n",
      "Epoch 6: train loss: 0.4643898606300354\n",
      "Epoch 7: train loss: 0.43858593702316284\n",
      "Epoch 8: train loss: 0.4149189591407776\n",
      "Epoch 9: train loss: 0.3932076394557953\n",
      "Epoch 10: train loss: 0.37328004837036133\n",
      "Epoch 11: train loss: 0.35497528314590454\n",
      "Epoch 12: train loss: 0.338186651468277\n",
      "Epoch 13: train loss: 0.3228713572025299\n",
      "Epoch 14: train loss: 0.3087400496006012\n",
      "Epoch 15: train loss: 0.2956806421279907\n",
      "Epoch 16: train loss: 0.2835915982723236\n",
      "Epoch 17: train loss: 0.2723812460899353\n",
      "Epoch 18: train loss: 0.2619667947292328\n",
      "Epoch 19: train loss: 0.2522739768028259\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_train)\n",
    "    loss = criterion(y_pred.squeeze(), y_train)\n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 1.4507834911346436\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "after_train = criterion(y_pred.squeeze(), y_test) \n",
    "print('Test loss after Training' , after_train.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
